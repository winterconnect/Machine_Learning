# Machine Learning

> 지도학습과 비지도학습을 구분하고, 지도학습은 분류와 회귀로 나뉜다
>
> 머신러닝적인 통계 접근과 통계학적인 통계 접근은 다르다!



## (교재) Introduction to Machine Learning with Python

> 원리에 대한 설명이 적은 편이므로 보충자료로 설명할 것
> ### CHAPTER2 지도학습
>
> **중점**: 분류와 회귀
>
> - 선형모델 - 결국 신경망 모델로 연결된다(딥러닝 학습은 모두 지도학습이다)
>
> - 결정트리
>
> 
>
> ###  CHAPTER3 비지도 학습
>
> GAN 에서 다룰 예정(GAN을 다룰 것인가...?)
>
> - 군집모델 
> - 전처리(Pre-processing)
>
> 
>
> ### CHAPTER4 데이터 표현과 특성공학
>
> 특성공학(Feature Engineering)
>
> - 원핫인코딩
> - 스케일링
>
> 
>
> ### CHAPTER5 모델 평가와 성능 향상
>
> 수업 앞쪽에서 보게 될 것
>
> 
>
> ### CHAPTER6, CHAPTER8 다루지 않을 가능성 큼
>
> 알고리즘 체인과 파이프라인 (진행하지 않을 수도 있음)
>
> - 그리드서치
> - 하이퍼 파라미터 튜닝
>
> ### CHAPTER7 - 자연어처리 (딥러닝에서 볼 것)








# 'Machine Learning'의 큰 틀

>## Agenda
>
>1. Artificial Intelligence: AI는 무엇인가
>
>- 인공지능을 만드는 것의 의미
>
>
>
>2. Machine Learning(Gradient Descent): 학습 원리
>   - 학습을 시키는 "머신"이 무엇인가?
>   - **Gradient Descent**: 경사하강법
>
>
>
>3. Model Validation
>   - 모델 검증이 무엇인가 
>
>
>
>머신러닝으로 할 수 있는 것은 기본적으로 두가지 (지도학습 - 분류와 회귀): 숫자나 범주를 예측하는 것
>
>4. Regression Analysis: 연속적인 숫자를 예측하는 것
>5. Logistic Regression: 분류(범주, category). 범주예측
>
>
>
>결정트리
>
>6. Decision Tree
>7. Random Forest(Ensemble)
>
>
>
>비지도학습 알고리즘
>
>8. K-means Clustering
>9. Association Rules





## 1. Artificial Intelligence

> - 역사적인 배경
> - 사람이 만든 "지능"



지능을 한가지로 정의할 수 있느냐?

지능을 만든다면, 지능이 무엇인지 명확히 정의할 수 있어야 한다





### 1) Definition

- 인공지능이란 인공장치(=기계)들의 **지능을 설계**하는 것(MaCarthy, 1956)

  - 인공장치: 사람이 만든 기계 (컴퓨터가 아닐 수 있음)
  - 인공장치가 인간의 지능을 모방하는 것

  - 물리적인 행동이 아니라, 논리적인 행동(지능)을 모방하는 것

  - 생각하고 판단하는 게 아니라, 사람이 해야하는 일을 대신해주는 것

    예) 계산기 

  

  인공지능이 엄청난 것이라고 생각하지 말자

  사람이 가진 지능의 아주 작은 부분을 구현하는 것

  

- 지능(Intelligence): 인간이 행하는 지적 작업의 주체
  - 지적작업: 근육이 아니라 인간의 두뇌활동에 의해 이루어지는 작업
  - 생명체가 생존 환경의 변화에 적응하기 위해 인지적 기능을 변화시키는 능력



### 2) History

#### (1) 1943년(여명기): 사람의 두뇌가 동작하는 것과 비슷한 기계를 만들 수 없을까?

- 두뇌 논리회로 모델링(McCulloch & Pitts)
- 1940년대 컨셉은 나왔지만, 실체화시킬 기술이 없어 이제서야 Deep learning으로 이어짐



#### (2) 1956년(태동기)

- 다트머스 회의에서 AI 용어 탄생



#### (3) 1956년 ~ 1970년(1차 인공지능 붐)

- 수동적 대화시스템

- 지능을 "기호처리"로 정의함

  - 언어는 기호이고, 기호의 교환을 통해 의사소통(communication)이 가능해질 것이라고 생각

    예) 챗봇

- pc도 없던 시절...



- 1971년~1979년(1차 빙하기): 프레임(고려해야할 범위)의 문제

  - 우리가 대화를 하는 데 있어서 생각보다 고려해야 할 것이 너무 많구나!

    예) '아침 드셨어요?'

    ​			사람은 '오늘 아침'으로 이해 가능(언어지능이 발달했으므로)

    ​			기계는 명확하게 설명해주어야 함

  - 이유: 컴퓨터 기술의 한계, 너무 어려운 주제...



- 1976, 최초의 pc 등장과 컴퓨터의 보급화
  - 지능을 다시 정의해보자
  - 기호의 교환이 아니라, 지능은 지식(knowledge)이다



#### (4) 1980년~1995년(2차 인공지능 붐)

- 전문가시스템(expert system) 활용(지능은 지식)

  예) 의사의 지식을 컴퓨터에 전달하고, 필요할 때 전문가에게 물어보는 것처럼 컴퓨터에 물어보자



- 1998년~2009(2차 빙하기): 지식획득 병목의 문제

  - 우리의 지식은 20%의 '형식지'(표현해낼 수 있는 지식)와  80%의 암묵지(남에게 정량적으로 명확하게 표현할 수 없는 지식)로 이루어져 있다

  - 80%의 암묵지를 컴퓨터에 제대로 전달할 수 없으며, 20%의 형식지만으로는 제대로 된 의사결정을 할 수 없다

  - 수집해서 정량화시켜 전달시키는 것이 어려움

    

#### (5) 2010년~ 현재(3차 인공지능 붐): 빅데이터와 딥러닝(지능은 학습)

- 사람의 지능은 어디에서 왔을까?

  예) 사람이 태어나 빈 공간에 혼자 두고 키운다면, 말을 할 수 있을까?

- 우리는 어떻게 말을 할 수 있게 되었을까?

  - 지속적으로 데이터(엄마, 아빠)가 들어오고, 패턴을 찾아내어 성대로 내보내게 됨

- 왜 그전에는 지능을 학습이라고 생각하지 못했나?

  - 컴퓨터가 학습하려는 디지털 데이터가 없음
  - 1970년대 pc 보급으로 디지털데이터 축적, 인터넷 연결과 스마트폰으로 데이터가 폭발적으로 증가하며 컴퓨터를 학습시킬 충분한 디지털데이터를 확보할 수 있게 됨

- 데이터를 주면 컴퓨터가 알아서 배울 수 있게 되겠구나!

- 그래서 "학습"(learning)이 붙게 됨: 디지털 데이터를 집어넣어서 특징을 찾게 하는 것이구나!

- 정형/비정형이든 컴퓨터에게 디지털데이터를 제공할 수 있는 이 시대에 자동으로 특징을 집어넣을 수 있게 됨

- 지능을 사람이 만들어서 집어넣는 것이 아니라, 컴퓨터가 데이터로 스스로 배울 수 있게 하려는 시대



---

우리는 지금 3차 인공지능 붐의 시대에 살고있다

---





### 3) AI Type

#### (1) 약한 인공지능(A**N**I: Artificial **Narrow** Intelligence)

- Narrow: 많은 양의 데이터를 처리하여 **특정 기능**만 수행
- "특화 지능": 특정기능에서는 사람보다 잘하지만, 모든 지능을 갖지는 못함
- 데이터를 가지고 기계가 특정 지능을 학습하도록 하는 것이 지금의 "약한 인공지능 시대"



#### (2) 강한 인공지능(A**G**I: Aritifical **General** Intelligence)

- 사람처럼 생각하고 판단하는 **범용** 인공지능
- 아직까지 존재하지 않는다
- 인간의 Natural Intelligence(자연지능)와 흡사



- 특이점(Singularity): 인간과 인공지능 사이의 임계점

  - 레이 커즈와일: 2045년에 특이점이 온다고 예측

  

#### (3) 슈퍼 인공지능(ASI: Artificial Super Intelligence)

- 특이점을 지나 인간의 지능을 넘어선 인공지능 - Transcendence(초월성)



기계가 인간을 공격할 것인가?

기계에게 가장 훈련시키기 어려운 것이 욕망

기계가 인간을 공격하는 이유는 1) 버그 2) 나쁜사람이 나쁜 용도로 사용해서



기술적인 부분도 중요하지만, **인공지능 윤리**가 대두됨





### 4) Turing Test

- Alan Turing, 1950
- 컴퓨터가 지능을 가지고 있는지 여부를 조사
- 질문자가 **인간과 컴퓨터에게 같은 질문**을 하여 **인간의 답**과 **컴퓨터의 답**을 **구분할 수 없으면** 컴퓨터가 지능을 갖고 있는 것으로 볼 수 있음 

- CAPTCHA(Completely Automated Public Turing test to tell Computers and Humans Apart): 사람인지 기계인지를 구분하는 장치





현재 컴퓨팅의 시스템을 "폰 노이만"이라고 한다



폰 노이만(맨하탄 프로젝트): 앨런 튜링의 지도교수

- 야사에서는 폰 노이만이 앨런 튜링의 아이디어를 훔쳐 컴퓨팅 시스템을 만든 게 아닌가 하는 썰이 있다



앨런 튜링: 게이, 당시 영국은 동성애가 불법. 화학치료를 받다 사과에 독을 묻혀서 먹고 자살



스티브 잡스: 앨런 튜링에게 바치는 로고가 아닌가? 하는 야사가 있음

- 깨물어먹은 사과. 무지개빛





### 5) Best Practices & Issues

#### (1) Technologies

- 딥러닝
  - 사물인식(object detecting): 명사만 알면 됨
  - 사물인식 및 이미지 보정
  - 시각장애인 주변 설명(image captioning): 동사, 형용사까지 알아야 됨
  - AI Reconstruct Photos(GAN 생성모델)
  - Deep Fake(GAN)
- 강화학습
  - OpenAI Solving Rubik's Cube(강화학습)

* 머신러닝은 "통계분석"에 가깝다

- Amazon GO
- ZOZOSUIT
  - 신체사이즈를 측정(시각인식), 사이즈가 맞는 옷을 추천

- etc
  - 투자분석 리포트: 애널리스트 15명이 한달 간 해야할 분석작업을 5분만에 처리
  - IBM Watson: 한해동안 발표되는 암 관련 논문 44,000개
  - 영화추천: 영촤 출연 배우 중 고객이 선호하는 배우 이미지로 광고 노출
  - 콘텐츠 생성: Google AutoDraw



#### (2) Issues

- Amazon Alexa: 아마존 인형의집 사건
- NIA Special Report
  - 인공지능 악용에 따른 위협과 대응 방안
  - 적대적 스티커(Adversarial Patch)



#### (3) Industrial Applications

- 제조업
- 자동차
- 소매업
- 금융업
- 운송업
- 헬스케어
- 엔터테인먼트: 맞춤형 개인화 서비스





## 2. Machine Learning (Gradient Descent)

> AI를 만드는 두가지 방식: ML/DL
>
> Intelligence(사람의 지적인 작업)
>
> 예측(Predict) : 과거의 데이터의 패턴이 미래에도 나타날 것이다
>
> - Regression(회귀): 수치를 예측
> - Classification(분류): 범주를 예측



### 1) ML에 대한 일반적인 설명



#### What is learning?

- 행동의 변화
- 학습 전 / 학습 후



- 학습이란 "어떤 **1) 작업**에 대해 **2)특정 기준으로 측정한 성능**이 **3)새로운 경험**으로 인해 향상되었다면, 그 프로그램은 어떤 작업에 의해 **4)향상**되었다면, 그 모델은 어떤 작업에 대해 특정 기준의 관점에서 새로운 경험으로부터 '배웠다'라고 말할 수 있다." <Tom M.Mitchell, 1998>

  1) 작업: 예측/분류

  2) 특정 기준으로 측정한 성능: 정량적

  ​	측정

  - 정성적 예) 크다, 많다, 좋다, 빠르다

  - 정량적 예) 2개 틀림, 30보다 작음

  3) 새로운 경험: parameter update

  4) 향상: 측정한 성능에 긍정적인 영향을 줌 (틀린 것이 줄고 맞은 것이 늘었다)



- 학습 이후 새로운 데이터에 대하여 학습된 내용으로 처리하는 것



#### 우리는 왜 학습하는가? 

- "긍정적으로" 변화하려고 (학습 전 후 행동의 변화, 사고의 변화)

- 변화(Change)는 좋을수도, 나쁠 수도 있음 (=Risk)
  - Positive
  - Negative



#### Machine Learning에서 "Machine" 은 무엇인가?

- 컴퓨터가 아니다.
- Computer는 H/W. 컴퓨터"로" 학습하는 것(컴퓨터는 도구)



예) 파란색, 빨간색 집단 분류

​	그래프에 그리면 좌표로 표현 가능하고, 일차방정식(y = ax + b)으로 구분선을 그리는 것이 가능해진다



- Machine을 **수학의 함수f(x) (y = ax+b)**로 생각하자

- 무엇을 학습시키는가? a와 b (=모델에서는 a와 b를 Parameter라고 한다)

- a와 b를 변화(change)시키는 것

  = Parameters인 a와 b를 주어진 데이터에 최적화(Optimization) 시킨다

- 학습이 된 함수를 "모델"이라고 한다





#### Machine Learning에서 "Learning(학습)" 은 무엇인가?

- 변경의 대상: a와 b
- 제공된 데이터: x와 y



---

#### 따라서 Machine Learning이란,

- 사람이 해야 할 의사결정(예측)을 학습된 함수(모델)을 통해 인공지능이 대신하게 하는 것
- 정해진 함수를 주어진 데이터를 가지고 학습한다

---



#### Machine Learning Definition

- **1) 머신**이 **2) 코드**로 명시되지 않은 동작을 데이터로부터 학습하여 실행할 수 있도록 하는 알고리즘

  ​	1) 머신: function  2) 코드: application/program 

  - 데이터로부터 일관된 패턴 또는 새로운 지식을 찾아내 학습하는 방법
  - 학습된 알고리즘(model)을 적용하여 정해진 업무를 처리

- 현실세계의 다양한 문제를 수작업에 의한 프로그래밍으로 대응하기 어려움

  - 개발자가 만든 것 이상을 수행할 수 있다는 의미

- 학습할 수 있는 것과 학습할 수 없는 것을 구분하는 것이 중요



#### Machine Learning Example

- 사람(Developer)이 만든 알고리즘(Software) vs. 기계(Machine)가 학습한 알고리즘(Model)
- Spam e-mail vs. Ham e-mail
- 레이블이 반드시 필요 (레이블을 토대로 지도학습) - 그래야 검증이 가능해짐

- 이렇게 레이블을 붙여주는 것이 "데이터 댐" 사업





#### Machine Learning 구조

Machine Learning

​	Supervised Learning

​		: Develop predictive model based on both input(=x) and output(=y, label) data

​		Regression

​		Classification

​	Unsupervised Learning: Group and interpret data based on only input data(=x)

​		Clustering(군집, 연관)



#### Supervised Learning(지도학습)

> 우리가 배우는 machine learning은 다 지도학습이라고 생각하자!



- 데이터에 존재하는 특징(Feature)을 바탕으로 처리 (수치예측, 범주예측)
  - feature: 통계학의 변수(Variable)
- input(x)에 대한 output(y)을 제공: data(labeling)가 제공되어야 학습할 수 있다
- input(x) Data와 output(y) Data(label)의 **관계**를 학습





#### Algorithm

| 지도학습                           | 비지도학습                      |
| ---------------------------------- | ------------------------------- |
| 회귀분석(Regression Analysis)      | 주성분분석(PCA)                 |
| 로지스틱 회귀(Logistic Regression) | K-평균 군집(K-means Clustering) |
| 의사결정 나무(Decision Tree)       | 연관 규칙                       |
| 랜덤 포레스트(Random Forest)       |                                 |
| 신경망(Neural Network)             |                                 |



#### Relationship

>  AI > ML > DL

- ML: 정형(RDB) 데이터인 경우가 많음

- DL: 비정형(사진, 음성, 소설, sns 데이터)

---

쓰임새가 다르다고 생각하면 된다. 그러나 DL도 ML안에 포함된다.

---





### 2) ML에 대한 기술적인 설명 - Gradient Descent

> Gradient: 경사/기울기 Descent: 하강
>
> 우리가 하는 모든 학습의 원리
>
> - 학습: Parameter Update
> - parameter를 update하는 원리가 바로 "gradient descent"이다



#### Function

- y = wx + b
  - w: weight(가중치)
  - b: bias(편향)



#### Loss Function() for Regression Analysis

- 오차 함수, Loss Function(): **1) 실제값**과 **2) 예측값**의 차이(**3) 오차**)를 비교하는 지표

  1) 실제값: y

  2) 예측값: y_hat(모델에 의해 나온 y값)

  3) 오차: 실제값과 예측값 사이의 차이(= error/loss/cost)

  - 점과 선 사이의 거리 

  - 오차를 계산하는 수식을 수학적으로 표현하면: y - y_hat
  - loss function은 y에서 y_hat을 뺀 값들의 합, 모두 양수로 만들기 위해 제곱함: L(y, y_hat) = (y - y_hat)**2
  - 제곱한 값이 작을수록 좋은 예측모델



​		예) Model : y_hat = 2x + 1 vs. y_hat = 3x + 0

| y    | x    | y_hat           | Loss              | y_hat2        | Loss              |
| ---- | ---- | --------------- | ----------------- | ------------- | ----------------- |
| 3    | 0.9  | 2*0.9 + 1 = 2.8 | (3-2.8)**2 = 0.04 | 3*0.9+0 = 2.7 | (3-2.7)**2 = 0.09 |
| 9    | 3    | 7               | 4                 | 9             | 0                 |
| 15   | 4.8  | 10.6            | 19.36             | 14.4          | 0.36              |
| 21   | 7.2  | 15.4            | 31.36             | 21.6          | 0.36              |



- 모델 값을 바꿔본다 (파라미터 값을 바꿈 -- 학습)
- 첫번째 모델보다 두번째 모델의 전체 오차의 합이 적다 -- 더 나은 모델이라고 할 수 있다
  - 합은 너무 크므로 보통 평균을 MSE(Mean Squared Error)를 작게 만드는 방향으로 한다
- w와 b를 크게하거나, 작게하면서 오차를 줄여나간다



- b의 값에 따라서 MSE이 작아질수도, 커질수도 있다
- w는 고정되어 있고, b만 바뀐다고 생각해보자

  - MSE = (y-y_hat)**2 = (y-(wx+b))\*\*2 = (y-wx-b)\*\*2
  - MSE 안에도 사실상 b가 들어가있고, 그래프는 사실상 b에 대한 2차방정식이다
  - 우리는 MSE 최소지점의 b를 찾고싶은 것
  - 그것이 b를 업데이트하는 학습의 목표



- 오차함수의 그래프를 알고있다고 한다면 b를 줄여야하는지 늘려야하는지 알 수 있겠지만, 실제로는 그래프를 모른 채로 오차를 줄여나가야한다
- 그러므로 b에(2차함수에) 미분을 한다
  - 미분이란? 순간변화량(delta) (=접선의 기울기)
  - 2차함수를 미분하면 1차함수가 됨
  - b지점에서 미분을 하면 접선이 생기고, 접선의 기울기를 계산할 수 있게 됨



- 경사하강법: b 값에서 기울기값을 빼서 원래 있던 b값에 다시 할당
  - b - 기울기(Gradient)

---

"b 값에서 기울기(Gradient)만큼 빼주면, error가 줄어들(Descent) 것이야!"

#### 경사하강법:

어느 지점에서든 미분의 접선의 기울기값을 빼주면(+ 이든 - 이든) 오차가 적어지는 방향으로 자동으로 이동할 것이다 (자동으로 업데이트 할 수 있을 것이다)

---



#### 감마(Learning Rate, Step Size)

- 그러나 기울기값이 무척 커서 한번에 너무 많이 이동하게 된다면, 방향이 맞더라도 에러가 커질 수도 있다(부호가 의미하는 것은 방향이지 크기가 아니므로)

- 기울기 값을 작게 제어해주는 매커니즘이 필요 (감마, 0~1 사이 값)
- 기울기 값에 감마 값을 곱해준다. 천천히 학습하도록
- 감마: Learning Rate(학습율) or step size 라고 부르게 된다



#### MSE와 w, b

- b뿐만 아니라 w도 같이 움직여야 한다 (따라서 GD는 곡면에서 동작한다)
  - 공간상에서 최소값을 찾아가는 방향으로 동작하게 된다

- 제곱오차의 면적을 최소화시키는 방향으로 모델을 만들어간다(w와 b의 값을 update)



---

#### 정리

- 데이터 기반 학습에는 gradient descent라는 개념이 무조건 적용된다

- Gradient Descent 의 식
  - w = w - r * dw(delta w)
  - b = b - r * db(delta b)

- gradient descent를 통해 주어진 데이터에 최적화된 함수를 자동으로 만들어낼 수 있다

- 원래있던 값에서 경사값(Gradient, 미분값)을 빼주면 오차가 감소(Descent)할 것이다
- 너무 많이 이동하면 안되니까, 0~1 사이의 값 r(감마)을 곱해준다 -- hyper parameter 라고 부름

- w, b: 파라미터 r: 하이퍼 파라미터

---





- Regression Analysis(수치예측)
  - y: 연속형 데이터
  - y = wx + b



