# 데이터 과학을 위한 통계

R로 하나 파이썬으로 하나 똑같다

R을 이용한 통계적인 방식

이론에 중점



>1. Descriptive Statistics (기술통계)
>2. Probability (확률)
>  - 통계적확률을 하고 있으므로 확률의 개념이 필요하다
>3. Distribution Function (분포함수)
>4. Inferential Statistics (추측통계)
>   - 우리가 하는 통계는 모두 추측통계이다
>   - 표본의 특징으로 모집단의 특징을 추측하는 것
>5. Estimation (추정)
>6. Hypothesis Testing (가설 검정)
>   - 가설을 검정하기 위해서는 가설을 만들어야 함
>   - 카이제곱검정
>7. Correlation Analysis (상관관계 분석)
>8. Regression Analysis (회귀분석)



모집단(원래 데이터, population)

표본(샘플)

부트스트랩: 표본을 추출하는 방법



- 통계는 기술통계, 추측통계로 나뉘어진다

- 추측통계이므로 추정과 가설검정이 필요하고, 확률이 필요해진다



## 1. Descriptive Statistics (기술통계)

>- 기술통계란?
>  - 기술: "손으로 쓴다" - 어떤 것을 손으로 써서 관리한다
>  - 통계(Statistics)란? State - 국가. 국가를 통치하는 방식으로 사용했기 때문에 "국상학"이라고도 했다 
>- 국가를 통치하는 기초자료로 활용했다
>- 데이터를 수집해서 기록하는 일이 "통계학", "국상학"의 시작
>
>- 역사는 길지만, 학문적으로 정립된 것은 200년 정도



- 통계: 데이터를 수집 및 정리하여 **특징을 확인**하는 것

  - 내가 가지고 있는 데이터가 정말 전체의 데이터인가?
  - 우선은 내가 가지고 있는 데이터가 모집단(전체집합)이라고 생각하자.

- 통계학에서 최종적으로 알고싶은 것 -> **모집단의 특징**
- 특징: "중심화 경향치"(모여있는 정도) vs. "산포도"(떨어져있는 정도) 

  - 통계학에서 말하는 특징은 밖에 없다

- 모집단의 특징을 알기위해 데이터를 수집, 분석, 시각화

  - 특정집단에 관한 현상을 수학적으로 연산하여 기술
  - 특징을 기술(설명)할 수 있으면 어떤 사실을 객관적으로 표현 가능
- 하나의 수치로 결론을 내릴 순 없지만 결론을 향한 기반을 제공



### 1) 변수와 척도



#### 통계변수(Variable)

- 변수: 머신러닝의 X, input, feature

|                     | 통계척도(Scale)   | 연산          |
| ------------------- | ----------------- | ------------- |
| 범주형 변수(정성적) | 명목형(명명) 척도 | == , !=       |
|                     | 순서형(서열) 척도 | >, <, >= , <= |
| 수치형 변수(정량적) | 이산형(등간) 척도 | + , -         |
|                     | 연속형(비율) 척도 | * , /         |

- 명명(Nominal): 측정값의 같고 다름만 확인, 측정값 사이에 순서 없음
- 서열(Ordinal): 측정값 사이에 순서가 있지만, 간격이 동일하지 않음
- 등간(Interval): 측정값 사이에 순서가 있고, 간격이 동일/영점(0)의 의미 임의적
  - ex) 온도 (섭씨 10 == 화씨 50, 섭씨 20 == 화씨 68): 우리가 만들어놓은 것이라 더하기는 되지만, 섭씨의 0과 화씨의 0이 다르므로 섭씨가 두배가 된다고 해서 화씨가 두배가 되지 않는다
- 비율(Ratio): 절대영점(아무것도 존재하지 않는 상태) 존재/사칙연산 모두 가능
  - ex) 길이 (미터 10 == 인치 393, 미터 20 == 787): 절대영점이 있으므로 단위가 다르더라도 일정 비율로 늘어나게 된다



### 2) 중심화경향치(Measure of Central Tendency)

- 집단을 대표하는 값(**무엇을 중심으로 모여있는가?**)

  ex) 평균(mean), 중앙값(median), 최빈값(mode)

  

#### 평균: 전체 데이터를 더한 다음 데이터의 개수로 나눠준 값

- 뮤
- 대표값으로 사용하기 어려운 경우가 있다
- 이상치(Outlier): 평균에 급격하게 영향을 주는 것

#### 중앙값: 전체 데이터를 크기 순서로 정하여 순서상 중앙에 위치한 값

- 평균의 대안
- 짝수일 경우, 가운데 2개를 더해 2를 나눔

#### 최빈값: 전체 데이터에서 가장 빈번하게 관찰된 값 



### 3) 산포도(Degree of Dispersion)

- 집단 내 데이터가 흩어져 있는 정도

  ex) 범위, 사분위범위, **분산**, 표준편차

- 중심화경향치를 계산해야 산포도를 구할 수 있다



#### 범위: 수치형 연속변수 값의 최솟값과 최댓값 사이의 거리

- 범위 = 최댓값 - 최솟값
- **이상치(outlier)**를 포함





- 최솟값(Minimum, Q1 - 1.5 * IQR)과 최댓값(Maximum, Q3 + 1.5 * IQR)
  - 계산하는 식은 같지만 꼬리의 길이는 다를 수 있다
  - 데이터가 있는 곳까지만 꼬리를 그리므로
- 사분위범위: 0%(최솟값), 25%, 50%(중앙값, median), 75%, 100%(최댓값) 구간으로 나눈 사분위수에서 25%와 75% 사이의 값들(boxplot으로 표현)
- IQR(Inter-Quantile Range) = Q3 - Q1



- outlier를 포함했을 때, 포함하지 않았을 때 비교



#### 분산(variance/var)

- 시그마 제곱
- 편찻값(관측치 - 평균)의 제곱의 합을 데이터의 수로 나눈 값
- 편찻값으로는 흩어진 정도를 조사할 수 없음(음수, 양수가 각각 상쇄)
- 따라서 편찻값 제곱의 평균으로 계산
- 관측치들이 평균에서 평균적으로 얼마나 떨어져있는지 확인 (하나의 숫자로 표현)
  - 단점: 값이 너무 커짐



#### 표준편차(Standard Deviation/std)

- 시그마
- 분산의 **양의 제곱근**
- 분산에 루트를 적용하여 계산





불편분산

p.34

평균을 구할 때 전체 개수로 나눠줌(n개), 혹은 n-1개로 해주는가?

일반적으로 n-1개로 나눠준다. 왜?

통계에서 보려고 하는 평균 자체가 데이터의 특징을 확인하기 위함인데,

우릭 가진 데이터가 모집단의 데이터라면 (데이터를 다 가지고 있으면) n개로 나눠도 된다

그러나 우리가 모집단의 데이터를 다 가지고 있는 경우는 거의 없다

따라서 표본집단의 특징(평균이나 분산)을 가지고 모집단의 특징을 추정하는 것



가진 데이터는 표본집단(샘플)이다

표본집단의 특징(통계량)을 보기 위해서는 n이 아니라 n-1로 나눠야 한다

n으로 나누면 실제보다 표본집단에 편향된 값이 나오게 된다



그래야 실제로 







## 2. Probability



확률은 비율(ratio)이다. 어떤 비율로 굳어지는 것



### Statistics vs. Probability

- 통계: 데이터를 수집 및 정리하여 "모집단의" 1) 특징을 확인하는 것

  1) 특징: 중심화경향치, 산포도

- 확률: 특정한 조건 하에 어떤 사건(event)이 발생할 **가능성의 정도**
  - 조건이 있어야 함
  - 확률에도 여러가지가 있는데, 일반적으로 말하는 것은 수학적 확률
  - 수학적 확률: 어떤 사람이 계산해도 동일한 값으로 계산되는 확률
  - 주관적 확률: 관찰자의 주관에 따라 다르게 표현되는 확률 (믿기 힘든 확률)
  - **통계적 확률**: 1) 어떤 사건을 2) **독립시행으로 반복**했을 때 발생하는 확률
    - 1) 표본 추출: 
    - 2) 복원 추출
    - 



- 우리에게 필요한 것은 통계적 확률



- 우리가 확인하고 싶은 것은 모집단의 특징인데, 실제 우리가 모집단의 모든 데이터를 가지고 있는 경우는 없다

  ex) 선거. 모집단: 유권자

  여론조사 => 모집단의 특징 확인

  A, B, C 후보가 있다면

  모집단의 특징이 있을 것 (A, B, C 후보 중 한명을 선호할 것)

  600만~700만명에게 전부 여론조사를 할 수 없다

  따라서 표본집단을 뽑아서 표본집단의 특징을 확인함 (약 1000명 정도)

  통상 응답률이 10퍼센트 내외 ()

  모집단의 특징을 추측

  추측을 다른 말로 표현하면, 맞을수도 있고 틀릴 수도 있다는 것

  정확하게 특징을 확인할 수 있고, 아닐 수도 있음

  잘못된 특징을 확인하는 가능성이 존재 - 확률의 개념이 필요한 이유



### 확률의 공리적 정의

- 공리: 어떤 이론체계에서 가장 기초적인 근거가 되는 명제



- '모든 사건의 경우의 수'에 대한 '특정 사건이 발생한 빈도수'의 비율
  - 확률은 비율(ratio)
- 사건 A의 확률은 0 <= P(A) <= 1 범위를 가짐
  - 모든 사건의 발생확률이 더해지면 1이 되어야 함
  - 공사건 = 0
  - 전사건 = 1
- 일반적으로 각각의 원소에 모두 같은 가중치 부여
  - 주사위 1~6은 모두 같은 비율로 발생한다고 가정
- 사건 A의 각각의 원소에 서로 다른 가중치 부여
  - 홀수보다 짝수가 3배 더 발생하는 주사위
- 문제의 핵심: 홀수에 할당된 확률과 짝수에 할당된 확률이 다르다는 것
  - 홀수가 나올 확률: s, 짝수가 나올 확률: 3s
  - s + 3s + s + 3s + s + 3s = 1
  - 12s = 1
  - s = 1/12
- 사건 A: 4보다 작은 수가 나올 사건, A = (1, 2, 3} 5/12
- 가중치가 같은 확률을 보게 됨



### 용어정리(중요)

| 집합     | 확률         | 통계             |
| -------- | ------------ | ---------------- |
| 원소     | 원소         | 데이터           |
| 전체집합 | **표본공간** | **모집단**       |
| 부분집합 | **사건**     | **표본집단**     |
| 합집합   | 덧셈정리     | **모집단분포**   |
| 교집합   | 곱셈정리     | **확률변수**     |
| 차집합   | 독립         | **확률분포**     |
| 여집합   | 조건부확률   | **누적분포함수** |

- 표본공간: 나타날 수 있는 전체 사건

내용이 이어지는 흐름

- 표본공간 - 모집단 - 모집단분포
- 사건 - 표본집단 - 확률변수 - 확률분포





### 표본공간(Sample Space)

- 확률용어

- 집합의 전체집합, 통계의 모집단을 의미

- 통계적 실험에서 발생 가능한 모든 결과(원소)들의 집합

  예) 두 개의 동전을 던지는 실험 : S = {HH, HT, TH, TT} 명목형(이산형)

  예) 반지름이 3인 원주상의 모든 점 : S = {(x, y) | x^2 + y^2 <= 9 } 연속형



### 사건(Event)

- 표본공간(전체집합)의 **부분집합**

  예) 전체 제품에서 2개를 뽑아 품질검사를 수행 : S = {TT, TF, FT, FF}

  예) 합격이 1개 이상인 사건(Event) : A = {TT, TF, FT}

- **표본집단**과 같은 개념
- 사건은 표본공간(모집단)의 부분집합(표본집단)
- 사건은 표본공간 밖에서 만들어질 수 없음
- 확률적 사건: 표본공간에서 특정한 조건을 만족하는 결과(원소)들의 집합
  - **전사건(Total Event)**: S의 모든 원소를 포함하는 사건 (통계의 전수조사)
  - 공사건(Null Event): S의 어떤 원소도 포함하지 않는 사건
    - 공집합. 통계와는 별 연관이 없다
  - 여사건(Complementary Event): 사건 A에 포함되지 않는 S의 모든 원소
  - 합사건
  - 곱사건
  - 배반사건: 한쪽에서만 발생하는 사건



- 표본집단이 모집단의 특징을 가지고 있을거라고 가정
- Machine Learning의 Train이 전체집단의 특징을 가지고 있다고 생각하는 것과 비슷한 개념



### 확률변수

- 확률적 법칙(사건)에 따라 변화하는 값
- 확률변수를 표본집단으로 간주

- 이산확률변수: 확률변수가 유한하게 존재(이산형)

  예) 두개의 동전을 던지는 실험 (셀수있음), 4개 중 3개

- 연속확률변수: 확률변수가 연속적인 구간 내의 값을 취함(연속형)

  예) 반지름이 3인 원주상의 모든 점 (무한개, 셀 수 없음), 360도 중 120도

- 상태공간: 확률변수가 취하는 모든 값의 집합
  
  - 상태공간과 표본집단은 같은 개념
- 상태공간을 구성하는 **각 값이 발생할 수 있는 확률**이 존재
- 모집단 : {1,3,5,7,9} 숫자가 적힌 5장의 카드
  - 확률변수X: 두 장의 카드를 복원추출하여 나온 숫자의 합(확률적 법칙)
    - 조건1: 두장의 카드 뽑기, 조건2: 복원추출, 조건3: 더하기
  - 확률변수X = {2, 4, 6, 8, 10, 12, 14, 16, 18}
    - 나올 수 있는 모든 숫자들
    - 그 중 각각의 값들이 발생할 수 있는 확률이 존재 (2가 나타날 확률, 4가 나타날 확률, ... )



확률변수와 각 값들이 나타날 확률을 연결시켜놓은 것이 "확률분포"



### 확률분포(Probability Distribution)

>- 확률들의 모양
>- 이 모양을 알 수 있다면, 확률을 예측할 수 있게 된다



- 확률변수 값과 각 값이 발생할 수 있는 확률을 대응시킨 것
- 확률변수 X의 확률분포표



- 확률분포함수: 확률변수 값이 발생할 확률을 나타내는 함수

- 특정 확률변수의 확률분포함수를 알고있다면? 어떤 **사건이 발생할 확률**을 계산(또는 예측) 가능

  - 특정 확률변수가 발생할 확률(likelihood, 우도)을 구할 수 있다
  - 최대우도추정법을 통해 모델링하는 기법을 사용하게 됨

- 두 장의 카드를 뽑아서(이산형), 두 수의 합이 8 이상 14 이하일 확률은?

  P(8 <= X <= 14) = 4/25 + 5/25 + 4/25 + 3/25 = 16/25



---

확률변수는 표본집단이라고 생각하자

하지만 우리가 진짜 알고싶은 것은 표본의 특징이 아니라, 모집단의 특징임을 기억하자

---



확률분포표로 확률분포함수를 정의할 수 있다





## 3. Distribution Function



분포함수란? 두가지가 있다



### 확률밀도함수 vs. 확률질량함수



- 이산형: 확률질량함수
  - 면적은 1, 발생확률을 모두 더하면 된다 (모두 더하면 1이 된다)

- 연속형: 확률밀도함수(Probability Density Funcion)
  - 곡선의 적분을 통해 면적의 합을 구할 수 있다
  - 면적의 합을 구해야 한다(적분해줘야 함), 모두 더하면 1이 된다



y축: 상대도수

질량함수냐, 밀도함수냐에 따라서 함수형태로 





### 누적분포함수(Cumulative Distribution Function)

> 일반적으로 분포함수 라고 부른다



- 확률변수 X의 누적분포함수 또는 분포함수

  - 확률변수X에 대해서 확률변수 X가 특정값보다 작거나 같을 확률

  - 확률밀도함수(연속형) 그래프 곡선의 아래 구간의 면적

  - F(x) = P(X <= x)

    

- 다양한 형태의 곡선이 나타날 수 있다

  - 함수이므로 수식에 따라

  

- 누적분포함수 종류(이산/연속에 따라)

  - 이산형: 확률**질량**함수(PMF, Probability Mass Function)
  - 연속형: 확률**밀도**함수(PDF, Probability Density Function)

  

- 세부 종류

  - 이항분포(Binomial Distribution): 이산형
    - 어떤조건을 만족하면 질량이 아니라 밀도함수처럼 동작하기도 한다
  - 정규분포(Normal Distribution): 연속형
  - 표준정규분포(Standard): 연속형

  

- 모수(Parameter): 분포함수의 모양을 결정(평균, 분산)
  - 뮤, 시그마제곱이 바뀌면 모양이 바뀐다
  - 이것을 찾으면 분포함수를 그릴 수 있고, 특정값에 대한 확률을 알 수 있게 된다
  - 따라서 통계에서는 평균과 분산을 파라미터라고 부른다(찾아야 하는 값이므로)
  - "모수추정법"이라고 한다



---

함수는 모양을 갖고 있고,

분포함수의 모양에 영향을 주는 것이 평균, 분산이다

모집단의 평균과 분산만 알면 확률을 알 수 있게 되므로 이것을 찾아야 한다

---



분포의 종류는 매우 많지만, 1.이항분포, 2.정규분포, 3.표준정규분포 만 살펴본다





### 이항분포

> 기본적으로 질량함수, 시행횟수가 많아지면 밀도함수



#### 베르누이 시행(Bernoulli's Trial)

- 이항분포를 베르누이 분포라고도 한다

- 두가지 결과가 나타나는 확률실험을 베르누이 시행이라고 함

  - p의 확률로 원하는 결과가 나타나면 **'성공'**
  - (1 - p)의 확률로 반대의 결과가 나타나면 **'실패'**

- 베르누이 시행의 분포는 성공의 확률에 따라 결정

  - 성공확률 p가 베르누이 시행의 **모수**: Bernoulli(p)

    예) 확률변수 X의 베르누이 시행: 성공일 때 1, 실패일 때 0

    - 확률질량함수



#### 이항분포

- 질량함수형태로 나타나던 함수가 밀도함수형태로 동작하게 된다
- 베르누이 시행을 여러번 하는 것을 이항분포라고 하고, 
- 이항분포 모수(Parameter)
  - 성공확률: p
  - 시행횟수: n
  - 성공횟수: x
  - 모수가 성공확률(p), 시행횟수(n)





### 정규분포(Normal Distribution)

- 프랑스 수학자 드 무아브르(de Moivre)
- 성공확률이 0.5, 시행횟수 n이 매우 큰 이항분포가 어떤 함수와 비슷해짐
  - 함수의 모양이 **평균을 중심으로 좌우대칭인 종모양**을 나타냄
  - 종모양의 형태로 양끝이 느린 속도로 감소하지만 x축에 닿지 않음
  - n이 매우 커지면 x가 이산형(질량함수)이 아닌 연속현(밀도함수)처럼 다룰 수 있음
- 정규분포 모수(Parameter)
  - 평균과 분산으로 분포의 모양을 결정
  - 평균: 뮤, 분산: 시그마제곱
- 평균이 바뀌면 그래프의 위치가 바뀜(좌우로 움직임)
- 분산이 커질수록 그래프가 펑퍼짐해짐
  - 면적의 합은 무조건 1이 되어야 하므로 중심 값은 낮아지고, 가장자리 값은 높아지게 됨
- 면적의 합은 무조건 1



---

이항분포를 많이 반복하면

정규분포의 형태로 나타나게 되고

정규분포의 모수는 평균, 분산이다

---



어떤 데이터가 정규분포의 형태를 띈다면 표준화할 수 있다



### 표준정규분포(Standard Normal Distribution)

- 평균 0, 분산 1인 정규분포
  - Z = N(0, 1)
- 확률변수 X의 평균이 뮤이고 분산이 시그마제곱인 정규분포를 따른다면
  - 표준변환(Z-transform)에 의해 정규분포를 표준정규분포로 변환 가능

  - 표준정규분포 모양 안에서 값들을 구해낼 수 있게 된다





---

#### 정리

- 확률변수에서 확률분포로 바뀌었고,
- 누적분포함수로 바뀐다
- 누적분포함수는 질량함수, 밀도함수가 있다
- 우리가 시행하는 횟수를 높이면 정규분포 형태를 가지게 된다



- 확률변수라는 것은, 확률적 법칙에 따라 변화하는 값(샘플링에 따라 달라질 수 있음)
- 확률변수에 확률을 연결시킨 것을 확률분포표라고 한다
- 확률분포표를 함수로 표현하기 위해 확률분포함수를 사용하게 되는데, 이산형(질량)/연속형(밀도) 가 나올 수 있다
- 이 함수 아래쪽 면적의 합은 1이 된다
- 누적분포함수를 통해 확률을 추정할 수 있게 될 것이고,
- 이항분포를 베르누이 분포라고 하는데,
- 이것을 반복하면 점차 평균을 중심으로 좌우가 대칭인 종모양형태가 된다
- 모수는 시행횟수와 성공확률
- 질량형이 밀도형처럼 동작하게 되고, 그 모양에 영향을 주는 것은 평균과 분산이 된다
- 정규형을 따른다면 표준화를 통해 표준정규분포로 변환 가능하다

---





### 기타분포



#### t-분포(Student's t-Distribution)

- 영국 통계학자 고셋: 작은 크기의 표본 연구논문 발표
  - 자유도가 증가할수록 표준정규분포와 비슷해짐

- 표본의 개수가 적을 때 사용
- 정규분포보다 조금 눌린 형태



#### 카이제곱분포

- 제곱이므로 마이너스가 없다
- 자유도가 모수로 모양을 결정
  - 자유도가 증가할수록 평균을 중심으로 좌우대칭 형태를 가짐



#### F-분포

- 분산도 제곱이므로 마이너스가 없다
- 자유도가 두개
- 두 집단의 분산을 비교할 경우 유용하게 사용 가능





#### 통계와 머신러닝의 가장 큰 차이점

통계학은 분포를 가정하고 들어가고, 그 모양에 따라 검증한다

머신러닝은 분포를 가정하지는 않고 데이터의 특징을 찾아내려고 한다



전체데이터를 알 수 없으니, 부분 데이터를 통해 전체 데이터를 추측함

모집단의 분포의 평균과 분산이 존재할 것이고, 표본집단 특징(평균, 분산)으로 추측하면 맞을까? 라는 것

맞을수도 있고, 틀릴수도 있지만 웬만하면 맞는다는 것





전수조사를 할 수 있다면 기술통계를 하면 되지만,

불가능하다면 (대부분의 경우) 추측통계를 하게 된다





## 4. Inferential Statistics (추측)

> 표본집단의 특징을 통해 모집단의 특징을  추측하는 것





세상의 모든 분표가 정규분포라면, 쉽게 추정할 수 있지 않을까?

일부의 특징을 가지고 전체의 특징을 추측하는 연결고리

- 연결고리 1: 알고싶은 건 모평균, 모분산이지만 알수있는 것은 표본평균, 표본분산이다

  - 뽑히는 표본에 따라 달라질 수 있다
  - 달라질 수 있는 모든 경우를 갖고있는 것이 표본분포표 (사실상 이것도 불가능)
  - 표본 통계량을 표시(일반적으로 평균), 표본평균분포표

  

추측통계: 시간, 비용, 신뢰성 이슈

전수조사를 한다는 것은 굉장히 어렵다

ex) 인구주택 총조사 정도, 하지만 이것도 오차가 있을 수 있고, 사실상 어렵다







- 과학적인 방법(통계적 방법, 중심극한정리)으로 **표본의 특징**을 통해 **모집단의 특징**을 **추측**
  - 1)전수조사 데이터는 기술통계로 모집단의 2)특징을 바로 관찰 가능
    - 1) 모집단의 정보를 모두 알고 있음
    - 2) 평균, 분산
  - 표본 데이터를 1)관찰할 수밖에 없는 경우에는 표본의 특징만 확인
    - 이 과정에서 확률의 개념이 등장
    - 1) x, observation
- 확률분포: 정규분포(Normal Distribution)
  - 정규분포가 중요한 역할을 하게 됨
  - 표본평균분포가 정규분포





### 1) 모수와(Parameter) 통계량(Statistic)

> 주의) Statistics 가 아니라 Statistic 이다



#### 모수: 모집단의 특징을 나타내는 값

- 알수는 없지만 존재하는 값으로 우리가 알고자 하는 특징
- 모수의 종류: 평균(뮤), 분산(시그마제곱), 표준편차
  - "모평균", "모분산"



#### 통계량(Statistic): 표본으로부터 관찰되는 표본의 특징을 나타내는 값

- 실제 우리가 확인가능한 표본의 특징
- 통계량을 통해 표본의 모양을 묘사
- 통계량의 종류: 평균(X_bar), 분산(S^2), 표준편차(S)
  - "표본평균", "표본분산"
- 수집된 표본에 따라 통계량이 달라짐





---

우리가 알고싶은 것은 모평균, 모분산인데

우리가 알 수 있는 것은 현실적으로 표본평균, 표본분산밖에 없다

우리가 알 수 있는 특징을 가지고 우리가 알고싶은 특징을 "과학적인 방법으로" 추측해보자

과학적, 이것이 가능한가?

---



### 2) 표본분포(Sampling Distribution)

> 앞에는 없다. 새로 나온 개념
>
> 표본으로 뽑힐 수 있는 것들의 전체집합
>
> 



- 표본의크기가 n으로 정해졌을 때 **추출될 수 있는 모든 표본**으로부터 구한 통계량들로 구성된 확률분포

  ex) 모집단의 크기는 모르지만, 표본으로 10개를 뽑는다고 하면 표본의 크기(n) = 10

  ​	  10개씩 뽑힐 수 있는 모든 combination을 구함

  

  - 통계량의 종류는 여러가지가 있지만 일반적으로 평균을 사용
  - 통계량: 표본평균, 표본분산, 표본표준편차
  - 모수: 모평균, 모분산, 모표준편차

  

- 표본평균분포(Sampling Distribution of Sample Means)

  - 표본평균 값들을 확률변수로 하는 1) 확률분포
    - 1) 확률변수 값과 각 값들이 나타날 확률을 대응시킨 것

  

- ex) 모집단을 알고 있는 경우, 표본크기 2로 복원추출한다면

  - 우리의 모집단: 1, 3, 5, 7, 9
  - n = 2
  - 실제로 알고싶은 것: 모집단의 특징(모평균, 모분산)
    - 가정한 집단이므로 특징을 알 수 있다. 모평균: 5, 모분산: 8
    - 실제로는 알기 어려우므로 표본특징으로 추측
  - 1, 3, 5, 7, 9 중 2개를 뽑음 (모든 경우의 수: 11, 13, 15, 17, 19, 33, 35, .... 전체 가지수: 25개)
  - 25가지 경우의 통계량(일반적으로 평균)을 구함
  - 나올 확률을 확률분포와 연결시켜 확률분포표를 구함
  - X_bar의 분포가 나타나는 모양을 구함
  - 표본평균(25개) 들의 평균 = 5, 모평균과 같아짐

  

#### 표본평균의 평균

- 확률변수 평균: 기댓값(Expected Value)



- 표본분포를 알고 있을 때, 표본분포의 평균을 다시 구하는 것
- 
- 기대값: 원래 확률변수값 *표본평균의 합
- 같은 방식으로 표본평균들의 분산도 구해볼 수 있다
- 표본평균들의 분산은 4가 나오게 됨
- 4는 8/2
- 모분산을 표본의 크기로 나눠주면 표본평균분산이 된다
- 표본평균분산 * 표본크기 = 모분산
- 표본평균분산을 알면, 모분산을 알 수 있게 된다



- 현실적으로 가장 큰 문제: 표본평균분포를 만들려면 모집단을 다 알아야 하는데, 모집단을 알 수가 없다
- 사실상 표본추출은 한번만 한다
- 중요한 특징: 표본분포의 특징이 정규분포의 형태를 따른다
- 정규분포를 따른다는 게 어떤 의미



- 이 중 단 한번만 뽑는다면, 어느 값이 뽑힐 확률이 가장 큰가? 5 (모평균)
- 분포가 정규분포를 띈다
- 모집단의 분포는? 이산형 (1, 3, 5, 7, 9 한번씩)
- 표본평균도 이산형이지만, 정규분포의 형태를 띄고 있다





### 3) 중심극한정리(CLT, Central Limit Theorem)

> 앞의 모집단 5개, 표본집단 2개는 그냥 느낌을 보기 위한 예시일 뿐



- 모집단의 분포와 상관없이 모집단의 평균과 표준편차(=분산)가 존재하면서, 표본크기 n이 충분이 크면 (n >= 30), **표본평균분포**는 정규분포를 따름
  - 주의) 표본분포가 정규분포를 따르는 게 아니라, **표본평균의 분포가 정규분포를 따름**
  - 표본평균분포가 정규분포를 따르므로 표준화하여 사용 가능

- 표본크기

  - 중심극한정리에 따르면, 표본의 크기를 키울 필요가 있다 (비용문제)
  - 표본평균 분포에서 표본평균의 분산(시그마제곱/n)을 줄이는 것에 영향을 줌
  - 표본크기가 증가할수록 표본평균의 분산은 감소

  

- 표본평균분포로 모집단의 특징을 추정

  - n >= 30 크기의 표본 사용
  - 모집단의 분포에 상관없이 정규분포를 사용하여 추정 가능
    - "통계는 분포를 가정하고 들어간다"는 것이 이 의미
  - 표본의 통계량을 기반으로 모집단의 모수를 추정
    - 이것이 맞을수도 있고, 틀릴수도 있음
    - 따라서 "신뢰수준", "오차수준"이라는 것이 등장



추측통계를 통해 추측을 하고, 

모집단의 특징을 추측하는 것이 "추정"

- 점추정(point): 거의 불가능에 가까움
- 구간추정(interval): 일반적으로 구간으로 추정함 (모집단이 어느 구간 안에 포함되어 있을 것이다)





### 4) 표준오차(Standard Error)

- 표본분포의 1)변동성 측정지표
  - 1) 분산
  - 표준오차는 추정의 정확도를 알려줌
  - 표준오차가 작을수록 추정치가 더욱 정밀(Precision)하다는 것을 의미
  - 표본평균의 표준편차로 계산: 표본의 수를 키우는 수밖에 없음
  - 표본의 크기가 늘어나면 표준오차는 줄어들게 된다
- n 제곱의 법칙(Square Root of n Rule)
  - 표준오차와 표본크기 사이의 관계
  - 표준오차를 2배 줄이기 위해서는 표본크기를 4배 증가해야 함



### 5) 표본오차(Sampling Error)

> estimation을 했을 때 오차 범위 내다, 범위 외에 있다 --- 표본오차
>
> 샘플링에 의해 발생할 수 있는 에러. 샘플링을 얼마나 잘 했느냐?



- 구간추정(Interval Estimation)의 최대허용오차
  - 대표성: 모집단을 대표하는 표본을 구성하지 못하여 발생하는 오차
  - 1) 임계값 * 표준오차
    - 1) 정규분포: z값, 카이: 카이제곱, t분포: t값 
    - 임계값: 신뢰구간에 따른 값

- 편향(Bias)과 우연(Chance)에 의해 발생
  - 편향에 의한 오차 감소: 표본추출방법을 엄격하게 수행
  - 우연에 의한 오차 감소: 표본크기를 증가
    - 표본을 많이 뽑으면 우연에 의한 오차는 줄어든다
    - 여론조사보다 출구조사의 정확도가 높은 이유(출구조사의 샘플의 수가 훨씬 크다)



### 6) 표본추출

> 다양한 방식의 추출법이 있다



- 선거관련 여론조사
  - 정확한 지지유을 알기위해 유권자 전체를 조사하는 것은 불가능
  - 유권자 집단을 대표할만한 표본을 추출하여 유권자 전체의 특징 추출

- 확률표본추출법(Probability Sampling)
  - 모집단 전체를 조사하는 것은 시간, 비용, 신뢰성의 문제로 불가능
  - 분석자의 의도가 반영되지 않는 확률 기반의 표본추출 기법
  - 종류: 단순임의추출법, 계통추출법, 층화추출법, 군집추출법



#### (1) 단순임의추출법(Simple Random Sampling)

> 머신러닝에서는 Random Sampling을 주로 사용

- 가장 이상적인 방법
- 모집단을 대표하는 표본을 추출 가능



#### (2) 계통 추출법(Systematic Sampling)

- 모집단의 각 객체가 선택될 가능성이 같다는 가정은 동일
- 모집단 전체에 일련번호를 지정
- 모집단의 수를 표본의 수로 나눈 K값 계산
- 1부터 K사이에서 임의로 한개의 표본을 추출
- 이후 K씩 더해가면서 해당위치에서 표본을 추출



#### (3) 층화 추출법(Stratified Sampling)

- 모집단을 인구학적 특성(성별, 직업, 학력, 거주지 등)으로 그룹화
- 각 그룹에서 일정 크기의 표본을 선택하여 전체 표본을 구성
- 각 그룹 내에서 일정한 수만큼 표본으로 추출
- 각 그룹 내 표본 추출 방식은 단순임의추출법이나 계통추출법 사용



#### (4) 군집 추출법(Cluster Sampling)

- 그룹화를 진행하는 것은 층화추출법과 동일(그룹 = 군집)
- 군집간의 응답(특징)에는 차이가 없다고 가정
- 난수를 사용하여 하나의 군집을 선택
- 선택된 군집을 표본으로 사용



- 위의 방식 중 하나를 선택해 샘플링은 한번만 수행한다





## 5. Estimation (추정)

> 추정한다는 것: 틀릴수 있는 가능성 존재한다는 의미
>
> 에러 발생. 에러가 작았으면 좋겠지만...
>
> 추측한 값이 얼마나 신뢰할 수 있는지 정의



- 추정: 모집단에서 추출된 표본의 특징을 파악하여 모수를 유추
- 점추정(Point Estimation)
  - 표본으로부터 계산되는 하나의 추정값
  - 단점: 추출되는 표본에 따라 오류가 발생할 가능성이 큼
- 구간추정(Interval Estimation)
  - 점추정의 단점을 보완하기 위한 방법
  - 하나의 점(값)이 아닌 **모수의 참값**이 포함될 것으로 기대하는 구간



구간으로 추정하므로, 



### 신뢰구간(Confidence Interval)

- 모수의 참값이 포함될 것으로 기대하는 구간



중심극한정리에 의해 표본평균의 분포는 정규분포를 따르게 되고

정규분포를 따르더라도 평균과 분산은 다르므로 표준화해서 표준정규분포(Z-분포)로 사용할 수 있다



제일 높은 지점(평균)이 0, 실제 원하는 모수의 참값이 



면적을 적분하여 구하면

90%로 잡으면 구간이 -1.645 ~ 1.645

95%로 잡으면 -1.96 ~ 1.96

99%로 잡으면 -2.576 ~ 2.576



90%가 좋을까 99%가 좋을까?

0.90으로 내려가면 모수의 참값을 포함할 가능성은 낮아진다

0.99이면 모수의 참값을 포함할 가능성은 높다

그러나 구간이 넓은 것... "내일 우리매장 매출액은 1000원에서 백억 사이에요"

현실적으로 원하는 것이 아님

따라서 0.95를 쓴다

신뢰구간이 크면 맞을 가능성은 높지만,

 

표준평균의 분포에서(정규분포이므로) 어느정도 구간으로 모수가 추정되어 있을 것인지 정하는 것

일반적으로 95퍼센트 신뢰구간을 사용

표준정규분포롤 봤을 때, +-1.96

우리가 원하는 모수의 값이 이 바깥에 있으면 "오류"(에러)

구간 안에 들어있으면 제대로 된 것



우리는 표본을 한번만 뽑아서 계산해낸다







#### 95% 신뢰구간

- 표본추출 자체를 한번만 함
- 한번의 표본추출을 통하여 95% 신뢰구간을 계산 시 그 구간에 모평균이 **포함될 확률이 95%임을 의미하지 않음**
- 모집단에서 n개의 표본추출을 여러 번 실시하여 신뢰구간 추정 시 그 결과들 중 약 95% 정도가 모평균을 포함할 수 있음을 의미
- 한번 추출한 표본에서 95퍼센트로 포함하고 있는 것이 아니라, 샘플링을 100번하면 그 중 95번은 모평균을 포함하고 있을 것이라는 의미(5번 정도는 다를 수 있다)
- 통계적 확률: 어떤 사건을 **독립시행으로 반복**했을 때 발생하는 확률





- 표본의 특징을 가지고 추측한다면, 분모가 표본의 분산으로 바뀐다



### 신뢰수준(Confidence Level)

> 아래 면적을 퍼센트로 표시한 것



- 모수의 참값이 표본을 통해 구해지는 신뢰구간에 존재하는 확률
- 95%(1 - 0.05) : -1.96 <= x <= 1.96 
  - 왼쪽, 오른쪽 각각 0.025
- 99%(1 - 0.01) : -2.576 <= x <= 2.576 
  - 왼쪽, 오른쪽 각각 0.001



### 유의수준(Significance Level)

> 바깥쪽 틀릴 수 있는 면적. 확률



- 모수의 참값이 표본을 통해 구해지는 신뢰구간에 존재하지 않을 확률
- 알파 = 0.05(95% 신뢰수준의 유의수준), 0.01(99% 신뢰수준의 유의수준) 





모분산을 모르는 경우 모집단의 모평균 구간추정

표본에 대한 신뢰구간을 구할 때는 t-분포를 사용한다



분산을 구할 때는 카이제곱분포를 사용한다







---

#### 추정 정리

- 추측이 몇퍼센트의 신뢰수준을 가질것인가?
- 모집단에서 추출된 표본의 특징을 통해 우리가 알고싶은 모수를 유추하는 것
- 점으로 추정하는 것은 거의 불가능하므로, 포함된 구간을 추정하는 것이고
- 값이 들어있을 것 같은 구간을 "신뢰구간"으로 정의
- 신뢰구간에 따라 신뢰수준(면적)이 달라진다
- 신뢰구간 90, 95, 99 할지 정하는데, 일반적으로 95
- 분포에 따라 구간이 달라지게 되고,
- 일반적으로 표본을 사용하므로 표본평균분포의 95퍼센트를 정규분포, t, 등 다양한 분포를 사용할 수 있다

- 모수가 그 안에 들어있지 않을 확률을 유의수준이라고 한다
- 그 오차를 가설검정에서 보게될 것

---





## 6. Hypothesis Testing (가설검정)

> 이것이 통계적으로 의미를 가지느냐?
>
> 가설이 우리가 받아들일 수 있는 것인가?



가설은 두가지가 존재한다

- 귀무가설
- 대립가설



#### Black Swan Theory

> 기존: 모든 백조는 흰색이다 (= 귀무가설, 영가설)
>
> 새로운 이론: 모든 백조는 흰색이 아니다 (= 대립가설)



기존에 믿었던 내용이, 확률적으로 발생하기 어려운 사건이 발생하면서 전체를 바꿔버리는 시스템

희귀하고 비상식적인 일(극단값)의 기준을 어떻게 잡을 것인가?



- 1697년 네덜란드 탐험가 윌리엄 드 블라밍
  - 서부 오스트레일리아에서 기존에 없었던 검은 백조 발견
- 희귀하고 비상식적인 일이 발생하여 전체를 바꿔버리는 시스템
  - 전혀 예상할 수 없는 일이 실제로 나타나는 것
  - 일단 발생하면 엄청난 변화를 초래할 만큼 극심한 충격을 가져옴
  - 시간이 흐르면 몰랐던 사실을 명확하게 설명하기 위해 노력
- 극단값: 과거 경험으로 확인할 수 없는 일반적 기대 영역 바깥에 놓인 값



그 극단값을 명확하게 정의해야하지 않을까?



### 1) 우연의 산물 vs. 실제 능력

- 객관식 문제의 우연에 의해(찍어서) 맞출 확률

  - 2개의 보기(0.5) vs. 5개의 보기(0.2) vs. 10개의 보기(0.1): 문제를 정말 안다고 보지 않는다

  - 보기가 20개(0.05): 문제를 풀 수 있는 능력이 있다고 봄

  

- 피셔(Fisher)교수와 Tea Time

  > 귀무가설: 홍차/우유 구별해내는 능력이 없다
  >
  > 대립가설: 홍차/우유 구별해내는 능력이 있다

  

  - 밀크티 문제: 우유가 먼저인가 홍차가 먼저인가
  - 8잔의 밀크티로 검정(Test)
  - 8잔 중 6잔을 맞춘 경우...극심한 충격을 주는것인가?
  - 이항분포로 계산하면(홍차/우유),  약 11퍼센트
  - 기준이 애매함
  - 검정의 기준(Significance Level)이 필요: p-value
  - 아주 희귀한 사건 자체가 우연히 발생할 확률이 5퍼센트 이하라면, 극심한 충격으로 인정한다
  - 5% 보다 낮으면 귀무가설을 기각하고 대립가설을 채택하겠다
    - 귀무가설을 기각한다고 해서 대립가설을 채택하는 것은 아니다
    - 귀무가설을 기각한다, 고 하고 대립가설을 어느정도 인정해준다
  - 5% 보다 높으면 그대로 귀무가설을 통계적 사실로 가져간다

  

### 2) 가설검정

- 모집단의 특징에 대한 1)여러 주장들 중 어떤 것을 받아들일지 결정하는 과정
  - 1) 가설
  - 모집단 특징에 대한 주장인 가설에 대해 **표본**으로부터 얻은 정보를 바탕으로 어떤 특징을 채택할지 기각할지 판단함으로써 모집단의 현재 특징에 대해 결정하는 과정
- 가설검정 결과 우연에 의해 발생할 확률(p-value)이 기준(유의수준)보다 낮은 경우 **기존의 주장을 버리고 새로운 주장을 받아들임**
  - 귀무가설을 버리고 대립가설을 받아들임
  - 귀무가설: A지점과 B지점의 평균매출에는 차이가 없음
  - 대립가설: A지점과 B지점의 평균매출은 다름(크거나 작을 수 있음)





---

우연에 의해 발생할 확률이 어떤 기준보다 작을 때,

기존의 내용을 버리고 새로운 내용을 받아들인다

이것을 Black Swan Theory

기존 값을 버리는 기준은 p-value

우연에 의해서 나타날 확률이 5퍼센트보다 낮으면 이것은 우연이 아니라고 받아들여야 한다

---





### 3) 가설검정 4단계

- 1단계: 가설수립
- 2단계: 표본으로부터 검정을 위한 통계량 계산
- 3단계: 가설 선택의 기준 수립
- 4단계: 판정 + 해석

이렇게 검정된 결과는 "통계적으로 의미를 갖는다". "통계적 사실"이다



#### 1단계 - 가설수립

> 귀무가설에 Advantage 를 준다

- 새롭게 확인하고자 하는 모집단 상태에 대한 가설

  - 귀무가설(영가설, Null Hypothesis)

  - 대립가설(연구가설, Alternative Hypothesis)

    

- 두 가설 중 새롭게 확인하고자 하는 가설이 대립가설(연구가설)

  - 가설검정에서는 귀무가설이 참이라는 가정 하에 검정을 진행
  - 귀무가설(H0): 신입생의 수능점수 평균은 386점이다
  - 대립가설(H1): 신입생의 수능점수 평균은 386점이 아니다 (높다, 낮다)



#### 2단계: 표본으로부터 검정을 위한 통계량 계산

- 검정통계량(Test Statistic)
  - 모수의 특성이 이루는 표본분표로부터 표본을 통해 관찰된 통계량
  - 검정통계량의 계산은 '귀무가설이 참'이라는 가정 하에 실시
  - 귀무가설 채택 및 기각 여부를 확인하기 위해 사용

- 판정단계에서 이 가정을을 유지할 것인지의 여부를 결정
  - 귀무가설이 참이라는 가정을 받아들일 수 없을 때 기각(Reject) : 0.05보다 작음
  - 귀무가설이 참이라는 가정을 받아들일 때 채택(Accept): 0.05보다 큼

  

#### 3단계: 가설 선택의 기준 수립(유의수준과 기각역)

- 두 개의 가설 중 하나를 선택할 때 발생할 수 있는 오류

  - 제1종 오류: 귀무가설이 사실인데 대립가설을 선택하는 오류(유의수준 알파 오류)
  - 제2종 오류: 귀무가설이 거짓인데 귀무가설을 선택하는 오류(유의수준 베타 오류)
  - 제1종 오류와 제2종 오류는 Trade-Off
- 일반적으로 95퍼센트 수준으로 정함
- 유의수준

  - 제1종 오류를 범할 확률의 최대 허용 한계: 5퍼센트
  - 유의수준이 알파인 검정: 제1종 오류를 범할 확률이 알파 이하인 검정
- 기각역
  - 기각역의 면적의 합은 유의수준 알파가 됨
  - 양측검정 vs. 단측검정



#### 4단계: 판정

- 검정통계량과 유의확률로 귀무가설의 채택 여부를 판정
- 기각역: 축 위의 지점 (0.05인 곳) 
- 유의확률과 유의수준을 이용한 판정
  - 유의확률이 유의수준보다 크면 귀무가설 채택
  - 유의확률이 유의수준보다 작다면 귀무가설 기각





### 4) 가설검정 실습



### 1. t-Test

- 일반적으로 모집단의 모평균이나 모분산은 알 수 없음
- 표본표준편차로 표본집단을 표준화를 하면, 표준정규분포가 아닌 t-분포를 따름
- 표본표준편차로 변환한 t-분포를 사용하기 때문에 t-Test라고 함



#### 1) 단일집단 t-Test

##### (1) Hypothesis

- 귀무가설: 스낵의 평균무게는 50이다.
- 대립가설: 스낵의 평균무게는 50이 아니다.

##### (2) 단일표본

##### (3) t-Test

- 단일표본 t-Test
- p값이 0.05보다 작기 때문에, '스낵의 평균무게는 50이다'는 귀무가설을 '기각'
  - 집단의 모평균의 차이는 통계적으로 유의미함



#### 2) 두 집단 독립표본 t-Test

- 두 표본집단을 구성(수집)할 때 독립인 경우
  - 독립표본: 무작위로 남자 100명과 여자 100명을 뽑아 두 집단을 비교
  - 대응표본: 부부 100쌍을 뽑아 남편 100명과 아내 100명의 두 집단을 비교
- 논리
  - 모평균과 표본평균은 다를 수 있지만 차이가 크지 않을 것
  - 두 표본집단의 평균의 차이는 0이 아니지만, 큰 차이는 보이지 않음
  - 만약, 두 표본평균의 차이가 크다면, 귀무가설이 맞지 않을 수 있음
  - 따라서, 두 표본평균의 차이나는 정도에 대한 가능성(확률)을 계산하여 판단
  - 

##### (1) Hypothesis

- 귀무가설: 두 집단의 모평균이 동일하다
- 대립가설: 두 집단의 모평균은 동일하지 않다



##### (2) 독립표본

- Male과 Female 표본집단이 무작위로 추출



##### (3) Welch's t-Test

- 그냥 t-Test를 쓰려면 두 집단의 등분산성을 검사해야 함
- 두 표본분산의 등분산성 검사 없이 진행 (일반적으로 Welch's t-Test를 사용함)
- 독립표본 t-Test에 적용(Independent)
  - 'equal_var = False'
- p값이 0.05보다 작기 때문에, '두 집단의 모평균이 동일하다'는 귀무가설을 '기각'
  - 두 집단의 모평균의 차이는 통계적으로 유의미함



##### (4) Effect Size

- 차이가 어느정도 나는지
- 두 집단의 평균 차이를 일정한 기준으로 표현
- 절대적이지 않으며, 0.2 정도면 작은 편, 0.5 정도면 중간, 0.8이면 큰 편



- Cohen's d: 일반적으로 사용

- Pearson's r



##### (5) 검증결과 해석

- 두 집단 Male과 Female에 대하여 독립표본 t 검증을 실시한 결과,
  - 집단 Male의 평균값(100)은 집단 Female의 평균값(108)보다 통계적으로 유의미하게 낮았으며 (p < 0.05),
  - 효과 크기는 중간 수준이었다(Cohen's d = 0.43, Pearson's r = 0.39)



#### 3) 두 집단 대응표본 t-Test

- 두 집단의 자료를 쌍으로 묶을 수 있는 경우 예) 남편과 아내, Before vs. After
- 쌍을 이루고 있는 두 값의 차이를 구함



##### (1) Hypothesis

- 귀무가설: 두 집단의 모평균이 동일하다
- 대립가설: 두 집단의 모평균은 동일하지 않다



##### (2) 대응표본

- 두 표본집단 Before와 After가 순서대로 짝지어 있음
- 백신을 맞기 전, 백신을 맞은 후
- 학원을 다니기 전, 학원을 다닌 후



##### (3) t-Test

- 대응표본 t-Test에 적용(Related)
- p값이 0.05보다 작기 때문에, '두 집단의 모평균이 동일하다'는 귀무가설을 '기각'
  
  - 두 집단의 모평균의 차이는 통계적으로 유의미함
  
  

##### (4) Effect Size

- 두 집단의 평균 차이를 일정한 기준으로 표현
- 절대적이지 않으며, 0.2 정도면 작은 편, 0.5 정도면 중간, 0.8이면 큰 편



##### (5) 검증결과 해석

- 두 집단 Before와 After에 대하여 대응표본 t검증을 실시한 결과,
  - 집단 Before의 평균값(100)은 집단 After의 평균값(108)보다 통계적으로 유의미하게 낮았으며(p < 0.05),
  - 효과 크기는 중간 수준이었다(Cohen's d = 0.48, Pearson's r = 0.43)





### 2. ANOVA Test

- ANalysis Of VAriance(ANOVA)
- 세 개 이상의 집단에 대한 평균을 비교할 때 사용
  - 기존 t-Test 사용시 과잉검증(Overtesting) 문제 발생
  - t-Test로 두 집단씩 짝지어 분석할 경우 분석횟수가 기하급수적으로 증가
  - 통계적 검증절차를 남용하여, 확률적 의사결정에서 발생 가능한 오류의 확률이 필요 이상으로 증가하는 문제
- 오류의 확률을 통제한 상태에서 전체적인 결과를 확인 가능
- 집단을 구성하는 변수(요인/Factor)가 두 개 이상인 경우, 상호작용 파악에 용이
  - 상호작용 파악
  - ex) 파란색/향, 녹색/향, 빨간색/향에 따라 판매량의 차이가 있을까?
  
  

#### 용어

- 요인(Factor): 집단을 구별하는 독립변수(ML 의 X)
- 수준(Level): 요인의 수준(예: '성별' 요인의 수준은 '남자'와 '여자')
- 상호작용: 한 요인의 수준에 따른 종속변수의 차이가 또 다른 요인의 수준에 따라 달라질 때
  - 두개가 만나 긍정적인 효과를 줄도 있고, 부정적인 효과를 줄 수도 있다
  - 긍정적: 강화효과
  - 부정적: 조절효과
- n-way ANOVA
  - One-way ANOVA: 요인이 1개인 분산분석
  - Two-way ANOVA: 요인이 2개인 분산분석
  
  

#### 1) One-way ANOVA

##### (1) Hypothesis

- 귀무가설: 모든 집단의 평균은 동일하다
- 대립가설: 적어도 한 집단의 평균은 다른 집단과 다르다 (집단이 3개 이상 있으므로)
- 평균이 같다면 끝
- 하지만 평균이 다르다면? 어느 집단 간 평균이 다른지 추가 분석



##### (2) 일원분산분석

- Pr(>F)가 p-value
- p-value값이 0.05 보다 작으면, 통계적으로 유의미한 차이가 존재
  - 뭔가 하나 다른 게 있다
- 검증결과 0.0159로 0.05 보다 작아서, 귀무가설 '기각'
- 구체적으로 어떤 수준(집단)이 차이가 있는지 확인하려면 **사후분석 (post hoc test)** 필요
- 유의미한 차이가 없는 경우, 사후분석 필요 없음



##### (3) 일원분산분석 가정

- 독립성: 자료의 추출은 독립적으로 이루어졌음
- 정규성: 자료의 모집단 분포는 정규분포를 따름
- 등분산성: 모든 집단의 모분산은 동일함

##### 독립성

- 자료수집이 Random Sampling 되었다면 만족하는 것으로 봄

##### 정규성

- 사피로 검증(정규성 검증)
  - 귀무가설: 모집단의 분포가 정규분포를 따른다
- 세 수준 모두 p-value 값이 0.05보다 큼
- 모집단의 분포가 정규분포를 따른다는 귀무가설을 '채택'

##### 등분산성

- 레빈검증, 바틀렛 검증
  - 귀무가설: 모집단의 모분산은 동일하다
- 두 가지 테스트 모두 p-value값이 0.05보다 큼
- 모집단의 모분산은 동일하다는 귀무가설을 '채택'



##### (4) 일원분산분석 사후분석

- post hoc test
  - 유의미한 검증결과 도출 시, 어떤 수준(들)에서 평균 차이가 나는지를 검증
  - 연구자의 사전가설(아이디어) 없이 ANOVA를 시행한 경우, 탐색적으로 평균 차이가 나는 수준(집단)을 살펴보기 위해 시행
  - 조합 가능한 모든 쌍에 대해 비교하여, 과잉검증으로 인한 FWER 증가
- FWER
  - Family Wise Error Rate
  - 여러 개의 가설 검정을 할 때 적어도 하나의 가설에서 1종 오류가 발생할 가능성
  - 가설검정을 많이 할수록 FWER은 증가
  - 유의수준을 보정하여 FWER을 0.05로 고정 후 검증



##### 4-1. Bonferroni Correlation

- 봉페로니 교정
  - 모든 집단을 짝지어 t-Test
- 'trt1'과 'trt2' 수준 간의 평균차이만 유의미함



##### 4-2. Tuckey's HSD

- 투키의 HSD
- 'trt1'과 'trt2' 수준 간의 평균차이만 유의미함



##### (5) 일원분산분석 결과 해석

- group 에 따른 weight의 평균 차이는 유의미함(p < 0.05)
  - 사후 분석을 실시 결과, 'trt1' 수준과 'trt2' 수준에서 유의미한 평균 차이가 있었음(p < 0.05)



#### 2) Two-way ANOVA

- Two-way ANOVA: 요인이 2개인 분산분석
- 요인 간 상호작용 파악이 주요 목적
  - 커피에 크림만 넣는 것, 시럽만 넣는 것, 크림과 시럽을 같이 넣는 것
  - 아무 효과가 없으면 상호작용 효과가 없음
  - 더 맛있어진다면, 강화효과
  - 더 맛없어진다면, 조절효과
- 다원분산분석 가정
  - 독립성
  - 정규성
  - 등분산성
- 주요 용어
  - 주효과: 다른요인(집단구분 변수)과 상관없이, 한 요인의 수준(집단)에 따라 효과가 유의미하게 달라질 때 '주효과가 있다'고 함
  - 상호작용효과: 한 요인의 수준에 따른 효과차이가 또 다른 요인의 수준에 따라 달라질 때, '요인들 간 상호작용이 존재한다'고 함



##### (1) Hypothesis

- 귀무가설: 모든 집단의 평균은 동일하다
- 대립가설: 적어도 한 집단의 평균은 다른 집단과 다르다



##### (2) 균형설계

- 각 집단과 조건별로 표본의 수가 동일한 경우
- 비균형설계의 경우 계산 방법이 달라짐



##### (3) 이원분산분석

- 'poison': p < 0.05로 유의미. 즉 poison의 수준에 따라 평균에 차이가 난다고 볼 수 있음
- 'treat' : p < 0.05로 유의미. 즉 treat의 수준에 따라 평균에 차이가 난다고 볼 수 있음
- 'poison:treat' : p > 0.05로 유의미하지 않음. 상호작용 효과는 발견하지 못함



##### (4) 비균형 설계

- anova_lm(TWA, typ = 3)
  - 'typ = 3' 옵션 추가



##### (5) 이원분산분석결과 해석

- 'time'에 대한 'poison'과 'treat'을 요인으로 하는 이원분산분석 실시 결과,
  - 'poison'의 주효과 유의(p < 0.05),
  - 'treat'의 주효과 유의(p < 0.05),
  - 'poison'과 'treat'의 유의미한 상호작용효과는 발견할 수 없음(p > 0.05)
- 만약 상호작용 효과가 유의미하다면,
  - 'poison' 1,2,3 집단 별 'treat'의 단순효과분석 수행





### 3. Chi-Squared Test

- 독립성 검정
- 명목형 데이터에 사용
- 관찰된 빈도가 기대된 빈도와 의미있게 다른지 여부를 검증
- 명목척도 자료의 분석에 사용



#### (1) Hypothesis

- 귀무가설: '가사노동의 종류'(행)와 '수행하는 사람'(열)은 독립이다.
  - 밥하는 사람, 청소하는 사람, 빨래하는 사람이 정해져있지 않다
- 대립가설: '가사노동의 종류'(행)와 '수행하는 사람'(열)은 독립이 아니다.
  - 어떤 일을 하는 사람이 정해져 있다



#### (2) 카이제곱검증 결과 해석

- p < 0.05 이므로, '가사노동의 종류'(행)와 '수행하는 사람'(열)은 독립이 아니다.







## 7. Correlation Analysis

> 두개의 변수가 같이 변하는 정도(공분산)를 보게 됨



### 상관관계 vs. 인과관계

- 상관관계: 원인과 결과가 불분명한 관계 (방향성이 없다)

  ex) 키와 몸무게

- 인과관계: 선행하는 변수의 변화가 다른 변수의 원인으로 작용하는 관계

  ex) 열을 가하면 물이 끓는다

- 담배와 폐암 (제조회사: 상관관계, 의사와 환자: 인과관계)

- 인과관계의 오류

  ex) 아이스크림의 판매량이 증가하면 익사자가 증가



### 상관분석

- 연속형(수치형) 데이터에 적용

  - 어떤 변수A의 변화가 다른 변수B에 어떤 영향을 주는가?

- 양의 상관관계 vs. 음의 상관관계

  ex) 소득수준이 높으면 수명이 길어지는가?

- 상관계수: 관계가 있는 경우 **얼마나 관계**가 있는지 나타내는 수치
- 일반적으로 모집단이 아닌 **표본의 상관계수**를 구함
  
  - 상관계수(r): 두 변수가 함께 변하는 정도를 -1 ~ 1 범위로 설명



두 변수의 공분산으로 설명



### 공분산(Covariance)

> 두 변수가 공통적으로 떨어져있는 정도

- 두 변수가 **함께 변하는 정도**를 나타내는 지표
- 공분산 부호
  - 공분산이 + : 두 변수가 같은 방향으로 변화
  - 공분산이 - : 두 변수가 반대 방향으로 변화
- 공분산 크기
  - 공분산 = 0 이면 두 변수가 독립 (서로 영향을 주지 않는다)
  - 공분산의 크기가 클수록 두 변수는 함께 많이 변화
- 단위에 따라 크기가 달라져 **절대적 크기로 판단이 어려움**
  - 공분산을 -1 ~ 1 범위로 표준화시킨 것이 상관계수



### 상관계수(Correlation Coefficient)

- 공분산을 두 변수의 표준편차의 곱으로 나눈 값
- 공분산을 표준화시켜놓은 것이라고 생각하면 된다





## 8. Regression Analysis

> 인과관계 분석
>
> 분석의 설계, 실험의 설계 과정에서 확인해야하는 것 3가지
>
> 상관계수: 회귀분석을 하기 전 먼저 체크해야 할 것



- 과거의 결과값을 기준으로 미래의 결과값을 예측하는 방법
  - 과거 결과값: 지난 달 강수량이 80일 때, 미세먼지농도가 45
  - 미래 예측값: 강수량이 75일 떄, 미세먼지농도를 예측
- 미래에 발생할 결과값이 "**과거의 평균으로 돌아간다(회귀)**"는 의미
- 회귀식: "y ~ ax + b"에서 최소제곱법으로 **회귀계수**를 계산
- y: 종속변수(반응변수), x: 독립변수(설명변수)
  - x의 값에 따라 y의 값이 결정된다는 의미



#### 인과관계 성립조건(3가지)

1. 두 변수(현상)의 동시 변화: x, y가 동시 변화

   - 공분산/상관계수 (상관관계는 인과관계를 성립시키기 위한 첫번째 조건이다)

2. 원인변수가 선행되어야 함

   - 원인변수(독립변수: x)는 결과변수(종속변수: y)보다 선행되어야 함
   - 광고비 지출(x)에 대한 매출변화(y)

3. **기타 요인의 부재**

   - 원인변수와 결과변수(독립변수와 종속변수)를 제외한 **기타 요인의 부재**

   - 오로지 원인변수(x)만이 결과변수(y)를 변화시켜야 하고, 제 3의 변수가 영향을 줄 가능성은 모두 제거되어야 함
   - 통제된 실험환경/분석환경/변수통제
   - 완벽하게 통제가 거의 불가능한 조건(우리가 고려하지 못한 상황이 발생할 가능성이 존재)



---

#### 통계학과 머신러닝의 차이

- 통계학은 머신러닝에 비해 효율적인 학문
  - 많은 데이터가 필요하지 않음
  - 샘플의 크기가 30개보다 많다면 표본평균의 평균은 정규분포를 따르게 될 것이고,
  - 샘플 수가 그보다 적다면 t분포를 사용하여 모집단의 특징을 추측

- 머신러닝과 통계는 접근법이 다르다

  - train, test split 같은 방법 없이 검증을 통계적 유의미함으로 함

  1. 전통적 통계분석: 전체데이터로 모델링 후 바로 predict
  2. 데이터마이닝: 데이터양이 많아지니 데이터의 특징을 확인하자
  3. 머신러닝

- 통계적 방법 중 데이터마이닝으로 넘어가면, 경계가 무의미해짐
  - train_test_split 하게 됨
  - 통계적인 관점에서 의미를 가지는지 확인함

---



- 최소제곱법(OLS: Ordinary Least Squares)
  - 회귀식의 파라미터값을 추정한는 방법 중 하나
- SST = SSR + SSE (설명 가능한 부분과 설명 불가능한 부분으로 이루어짐)
  - SST(Total Sum of Squared): sum(y - y_bar)**2
    - y의 전체 변동(y와 y_bar의 차이)
  - SSR(Regression Sum of Squared): sum(y_hat - y_bar)**2
    - 회귀직선으로 설명되는 변동
  - SSE(Error Sum of Squared): sum(y - h_hat)**2 
    - MSE와 식이 같음
    - 회귀직선으로 설명 불가능한 변동



#### 결정계수(Coeffieicnt of Determination)

- SST와 SSR의 비율을 보는 것이 결정계수
- R^2 설명력은 X로 설명할 수 있는 Y의 변동을 의미
- 1에 가까울수록 선형회귀모델의 설명력이 높다는 것을 의미
- R^2: 회귀직선으로 설명되는 변동/전체변동 = SSR/SST



#### 수정된 결정계수(Adjusted R^2)

- 독립변수의 개수가 증가함에 따라 R^2값이 증가하는 문제 발생

  - 따라서 다중회귀분석에서는 Adjusted R^2값을 사용
  - 독립변수의 개수 p를 분모에 위치시켜  R^2값이 증가되는 영향을 감소

- 1 - ((n-1) / (n-p-1)(1-R^2))

  - n: 샘플의 개수

  - p: 샘플 종류의 개수

  - p값을 분모에 적용하여 Adjusted R^2값을 보정

  - p가 늘어나면 뒤 분수의 값이 커지고, 전체 R^2의 값은 줄어든다

    

### 단일회귀분석

- 종속변수에 영향을 주는 독립변수가 1개인 경우
- formula = 아들의 키 ~ 아버지의 키




#### 상관계수 & 산점도
  - pearson 상관계수
  - 회귀선 시각화




#### 회귀계수 추정
  - 회귀직선:  y = ax + b
  - 회귀계수: a, b
  - 회귀계수를 계산하여 회귀직선을 구하는 것




#### 결정계수(R-Square)
  - 종속변수 예측값(Fitted Value)과 실제값(Actual Value)의 상관계수 제곱값
  - 회귀모델의 저갑도 평가(Measure)를 위하여 사용
  - 결정계수는 회귀모형의 설명력
  - 상관계수 범위: 0 <= R^2 <= 1
  - 독립변수의 개수가 증가하면 결정계쑤값도 증가
  - 다중회귀분석에서는 Adjusted R-Square 적용



#### 모델의 선형선

1) 예측값(fitted) 계산

2) 잔차(residual) 계산

3) 예측값과 잔차 비교

- 모든 예측값에서 잔차가 비슷하게 있어야 함
- 잔차의 추세: 빨간실선
- 빨간실선이 회색점선을 크게 벗어난다면 예측값에 따라 잔차가 크게 달라지는 것을 의미



#### 잔차분석

1) 잔차의 정규성

- 잔차가 정규분포를 따른다는 가정 검증
- Q-Q 플롯: 잔차가 정규분포를 띄면 Q-Q 플롯에서 점들이 점선을 따라 배치
- shapiro Test: p값이 0.05보다 작아 잔차의 정규성을 따른다는 귀무가설을 기각



2) 잔차의 등분산성

- 예측값이 크든 작든, 모든 값들에 대하여 잔차의 분산이 동일하다는 가정
  - 예측값(가로축)에 따라 잔차가 어떻게 달라지는지 시각화
  - 빨간실선이 수평선을 그리는 것이 이상적



3) 잔차의 독립성

- 회귀분석에서 잔차는 정규성, 등분산성 그리고 독립성을 가지는 것으로 가정
- 자료 수집 시 Random Sampling을 하였다면 잔차의 독립성은 만족하는 것으로 봄



4) 극단값

- Cook's distance: 극단값을 나타내는 지표





### 다중회귀분석

- 종속변수에 영향을 부는 독립변수가 여러 개인 경우
- formula = sepal_length ~ sepal_width + petal_length
- 기타요인 부재조건을 충족하기 상당히 어려움(외부요건의 영향을 많이 받음)



#### 다중공선성

- x가 두개 이상이면 나타날 수 있는 문제
- 독립변수 간의 강한 상관관계로 인하여 발생
- 회귀분석의 **결과를 신뢰할 수 없는 현상**

- 분산팽창요인(VIF: Variation Inflation Factor)
  - VIF값이 10 이상인 경우 다중공선성 문제 의심 (통상적으로)
- 다중공선성 문제해결: 독립변수들 간에 강한 상관관계를 보이는 독립변수 제거 후 모델링